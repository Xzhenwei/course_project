{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e147e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f123b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, average_precision_score\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "from util import ece, ParameterDistribution\n",
    "\n",
    "# Set `EXTENDED_EVALUATION` to `True` in order to visualize your predictions.\n",
    "EXTENDED_EVALUATION = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "678b9e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_solution_compare(dataset_train: torch.utils.data.Dataset, data_dir: str = os.curdir, output_dir: str = '/results/') -> 'Model':\n",
    "    \"\"\"\n",
    "    Run your task 2 solution.\n",
    "    This method should train your model, evaluate it, and return the trained model at the end.\n",
    "    Make sure to preserve the method signature and to return your trained model,\n",
    "    else the checker will fail!\n",
    "\n",
    "    :param dataset_train: Training dataset\n",
    "    :param data_dir: Directory containing the datasets\n",
    "    :return: Your trained model\n",
    "    \"\"\"\n",
    "    SCORE=()\n",
    "    for a in np.linspace(0.5,2,3):\n",
    "        for b in np.linspace(0.5,2,3):\n",
    "            param = [a, b]\n",
    "            # Create model\n",
    "            model = Model(param)\n",
    "#             model.get_param = param\n",
    "            # Train the model\n",
    "            print('Training model')\n",
    "            model.train(dataset_train)\n",
    "\n",
    "            # Predict using the trained model\n",
    "            print('Evaluating model on training data')\n",
    "            eval_loader = torch.utils.data.DataLoader(\n",
    "                dataset_train, batch_size=64, shuffle=False, drop_last=False\n",
    "            )\n",
    "            accuracy, ece = evaluate(model, eval_loader, data_dir, output_dir)\n",
    "            score = accuracy+ 3*(0.5- ece)**2\n",
    "            SCORE+=(score,)\n",
    "            print(score)\n",
    "        print('')\n",
    "    best = min(SCORE)\n",
    "    # IMPORTANT: return your model here!\n",
    "    return model, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3af93613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_solution(dataset_train: torch.utils.data.Dataset, data_dir: str = os.curdir, output_dir: str = '/results/') -> 'Model':\n",
    "    \"\"\"\n",
    "    Run your task 2 solution.\n",
    "    This method should train your model, evaluate it, and return the trained model at the end.\n",
    "    Make sure to preserve the method signature and to return your trained model,\n",
    "    else the checker will fail!\n",
    "\n",
    "    :param dataset_train: Training dataset\n",
    "    :param data_dir: Directory containing the datasets\n",
    "    :return: Your trained model\n",
    "    \"\"\"\n",
    "\n",
    "    # Create model\n",
    "    model = Model()\n",
    "\n",
    "    # Train the model\n",
    "    print('Training model')\n",
    "    model.train(dataset_train)\n",
    "\n",
    "    # Predict using the trained model\n",
    "    print('Evaluating model on training data')\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=64, shuffle=False, drop_last=False\n",
    "    )\n",
    "    evaluate(model, eval_loader, data_dir, output_dir)\n",
    "\n",
    "    # IMPORTANT: return your model here!\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8eb24df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"\n",
    "    Task 2 model that can be used to train a BNN using Bayes by backprop and create predictions.\n",
    "    You need to implement all methods of this class without changing their signature,\n",
    "    else the checker will fail!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, param):\n",
    "        # Hyperparameters and general parameters\n",
    "        # You might want to play around with those\n",
    "        self.num_epochs = 1  # number of training epochs\n",
    "        self.batch_size = 128  # training batch size\n",
    "        learning_rate = 1e-3  # training learning rates\n",
    "        hidden_layers = (100, 100)  # for each entry, creates a hidden layer with the corresponding number of units\n",
    "        use_densenet = False  # set this to True in order to run a DenseNet for comparison\n",
    "        self.print_interval = 100  # number of batches until updated metrics are displayed during training\n",
    "        self.param = param\n",
    "        # Determine network type\n",
    "        if use_densenet:\n",
    "            # DenseNet\n",
    "            print('Using a DenseNet model for comparison')\n",
    "            self.network = DenseNet(in_features=28 * 28, hidden_features=hidden_layers, out_features=10)\n",
    "        else:\n",
    "            # BayesNet\n",
    "            print('Using a BayesNet model')\n",
    "            self.network = BayesNet(self.param, in_features=28 * 28, hidden_features=hidden_layers, out_features=10)\n",
    "\n",
    "        # Optimizer for training\n",
    "        # Feel free to try out different optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train(self, dataset: torch.utils.data.Dataset):\n",
    "        \"\"\"\n",
    "        Train your neural network.\n",
    "        If the network is a DenseNet, this performs normal stochastic gradient descent training.\n",
    "        If the network is a BayesNet, this should perform Bayes by backprop.\n",
    "        :param dataset: Dataset you should use for training\n",
    "        \"\"\"\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=True, drop_last=True\n",
    "        )\n",
    "\n",
    "        self.network.train()\n",
    "\n",
    "        progress_bar = trange(self.num_epochs)\n",
    "        for _ in progress_bar:\n",
    "            num_batches = len(train_loader)\n",
    "            for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "                # batch_x are of shape (batch_size, 784), batch_y are of shape (batch_size,)\n",
    "\n",
    "                self.network.zero_grad()\n",
    "\n",
    "                if isinstance(self.network, DenseNet):\n",
    "                    # DenseNet training step\n",
    "\n",
    "                    # Perform forward pass\n",
    "                    current_logits = self.network(batch_x)\n",
    "\n",
    "                    # Calculate the loss\n",
    "                    # We use the negative log likelihood as the loss\n",
    "                    # Combining nll_loss with a log_softmax is better for numeric stability\n",
    "                    loss = F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y, reduction='sum')\n",
    "\n",
    "                    # Backpropagate to get the gradients\n",
    "                    loss.backward()\n",
    "                else:\n",
    "                    # BayesNet training step via Bayes by backprop\n",
    "                    assert isinstance(self.network, BayesNet)\n",
    "                    current_logits, log_prior, log_var_post = self.network(batch_x)\n",
    "                    nll_loss = F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y, reduction='sum')\n",
    "                    # reg_term = (log_var_post - log_prior)\n",
    "                    loss = nll_loss #+ reg_term\n",
    "                    loss.backward()\n",
    "                    # TODO: Implement Bayes by backprop training here\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Update progress bar with accuracy occasionally\n",
    "                if batch_idx % self.print_interval == 0:\n",
    "                    if isinstance(self.network, DenseNet):\n",
    "                        current_logits = self.network(batch_x)\n",
    "                    else:\n",
    "                        assert isinstance(self.network, BayesNet)\n",
    "                        current_logits, _, _ = self.network(batch_x)\n",
    "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
    "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item())\n",
    "\n",
    "    def predict(self, data_loader: torch.utils.data.DataLoader) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the class probabilities using your trained model.\n",
    "        This method should return an (num_samples, 10) NumPy float array\n",
    "        such that the second dimension sums up to 1 for each row.\n",
    "        :param data_loader: Data loader yielding the samples to predict on\n",
    "        :return: (num_samples, 10) NumPy float array where the second dimension sums up to 1 for each row\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.eval()\n",
    "\n",
    "        probability_batches = []\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            current_probabilities = self.network.predict_probabilities(batch_x).detach().numpy()\n",
    "            probability_batches.append(current_probabilities)\n",
    "\n",
    "        output = np.concatenate(probability_batches, axis=0)\n",
    "        assert isinstance(output, np.ndarray)\n",
    "        assert output.ndim == 2 and output.shape[1] == 10\n",
    "        assert np.allclose(np.sum(output, axis=1), 1.0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1775a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing a single Bayesian feedforward layer.\n",
    "    It maintains a prior and variational posterior for the weights (and biases)\n",
    "    and uses sampling to approximate the gradients via Bayes by backprop.\n",
    "    \"\"\"\n",
    "    def __init__(self, param, in_features: int, out_features: int, bias: bool = True):\n",
    "        \"\"\"\n",
    "        Create a BayesianLayer.\n",
    "        :param in_features: Number of input features\n",
    "        :param out_features: Number of output features\n",
    "        :param bias: If true, use a bias term (i.e., affine instead of linear transformation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        print(f'initialising bayesian layer of size {self.in_features} x {self.out_features}')\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # TODO: Create a suitable prior for weights and biases as an instance of ParameterDistribution.\n",
    "        #  You can use the same prior for both weights and biases, but are free to experiment with different priors.\n",
    "        #  You can create constants using torch.tensor(...).\n",
    "        #  Do NOT use torch.Parameter(...) here since the prior should not be optimized!\n",
    "        #  Example: self.prior = MyPrior(torch.tensor(0.0), torch.tensor(1.0))\n",
    "        a = param[0]\n",
    "        b = param[1]\n",
    "        self.weights_prior = MultivariateDiagonalGaussian(torch.zeros(out_features * in_features), a*torch.ones(out_features * in_features))\n",
    "        self.bias_prior = MultivariateDiagonalGaussian(torch.zeros(out_features), b*torch.ones(out_features))\n",
    "        assert isinstance(self.weights_prior, ParameterDistribution)\n",
    "        assert isinstance(self.bias_prior, ParameterDistribution)\n",
    "        assert not any(True for _ in self.weights_prior.parameters()), 'Prior cannot have parameters'\n",
    "        assert not any(True for _ in self.bias_prior.parameters()), 'Prior cannot have parameters'\n",
    "\n",
    "        # TODO: Create a suitable variational posterior for weights as an instance of ParameterDistribution.\n",
    "        #  You need to create separate ParameterDistribution instances for weights and biases,\n",
    "        #  but can use the same family of distributions if you want.\n",
    "        #  IMPORTANT: You need to create a nn.Parameter(...) for each parameter\n",
    "        #  and add those parameters as an attribute in the ParameterDistribution instances.\n",
    "        #  If you forget to do so, PyTorch will not be able to optimize your variational posterior.\n",
    "        #  Example: self.weights_var_posterior = MyPosterior(\n",
    "        #      torch.nn.Parameter(torch.zeros((out_features, in_features))),\n",
    "        #      torch.nn.Parameter(torch.ones((out_features, in_features)))\n",
    "        #  )\n",
    "#         small_const = 0.001\n",
    "#         large_neg_const = -10.\n",
    "        small_const = 0\n",
    "        large_neg_const = -4\n",
    "        self.weights_var_posterior = MultivariateDiagonalGaussian(torch.nn.Parameter(small_const * torch.ones((out_features*in_features))),\n",
    "                                                                  torch.nn.Parameter(large_neg_const *  torch.ones((out_features*in_features))))\n",
    "\n",
    "        assert isinstance(self.weights_var_posterior, ParameterDistribution)\n",
    "        assert any(True for _ in self.weights_var_posterior.parameters()), 'Weight posterior must have parameters'\n",
    "\n",
    "        if self.use_bias:\n",
    "            # TODO: As for the weights, create the bias variational posterior instance here.\n",
    "            #  Make sure to follow the same rules as for the weight variational posterior.\n",
    "            #print('Bias dist dim', out_features)\n",
    "            self.bias_var_posterior = MultivariateDiagonalGaussian(torch.nn.Parameter(small_const * torch.ones((out_features,))),torch.nn.Parameter(large_neg_const * torch.ones((out_features,))))\n",
    "            assert isinstance(self.bias_var_posterior, ParameterDistribution)\n",
    "            assert any(True for _ in self.bias_var_posterior.parameters()), 'Bias posterior must have parameters'\n",
    "        else:\n",
    "            self.bias_var_posterior = None\n",
    "\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Perform one forward pass through this layer.\n",
    "        If you need to sample weights from the variational posterior, you can do it here during the forward pass.\n",
    "        Just make sure that you use the same weights to approximate all quantities\n",
    "        present in a single Bayes by backprop sampling step.\n",
    "        :param inputs: Flattened input images as a (batch_size, in_features) float tensor\n",
    "        :return: 3-tuple containing\n",
    "            i) transformed features using stochastic weights from the variational posterior,\n",
    "            ii) sample of the log-prior probability, and\n",
    "            iii) sample of the log-variational-posterior probability\n",
    "        \"\"\"\n",
    "        # TODO: Perform a forward pass as described in this method's docstring.\n",
    "        #  Make sure to check whether `self.use_bias` is True,\n",
    "        #  and if yes, include the bias as well.\n",
    "        weights = self.weights_var_posterior.sample()\n",
    "        log_prior = self.weights_prior.log_likelihood(weights)\n",
    "        log_variational_posterior = self.weights_var_posterior.log_likelihood(weights)\n",
    "        bias = None\n",
    "        if self.use_bias:\n",
    "            bias = self.bias_var_posterior.sample()\n",
    "            bias_log_prob = self.bias_prior.log_likelihood(bias)\n",
    "            log_prior += bias_log_prob\n",
    "            log_variational_posterior += self.bias_var_posterior.log_likelihood(bias)\n",
    "        #print('bl', inputs, weights.reshape(self.out_features, self.in_features), bias)\n",
    "        return F.linear(inputs, weights.reshape(self.out_features, self.in_features), bias), log_prior, log_variational_posterior\n",
    "\n",
    "\n",
    "class BayesNet(nn.Module):\n",
    "    def __init__(self, param, in_features: int, hidden_features: typing.Tuple[int, ...], out_features: int):\n",
    "        super().__init__()\n",
    "        feature_sizes = (in_features,) + hidden_features + (out_features,)\n",
    "        num_affine_maps = len(feature_sizes) - 1\n",
    "        self.layers = nn.ModuleList([\n",
    "            BayesianLayer(param, feature_sizes[idx], feature_sizes[idx + 1], bias=True)\n",
    "            for idx in range(num_affine_maps)\n",
    "        ])\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        log_prior = torch.tensor(0.0)\n",
    "        log_variational_posterior = torch.tensor(0.0)\n",
    "        current_features = x\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            f = layer.forward(current_features)\n",
    "            new_features = f[0]\n",
    "            log_prior += f[1]\n",
    "            log_variational_posterior += f[2]\n",
    "            if idx < len(self.layers) - 1:\n",
    "                new_features = self.activation(new_features)\n",
    "            current_features = new_features\n",
    "        output_features = current_features\n",
    "        return output_features, log_prior, log_variational_posterior\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor, num_mc_samples: int = 10) -> torch.Tensor:\n",
    "        probability_samples = torch.stack([F.softmax(self.forward(x)[0], dim=1) for _ in range(num_mc_samples)], dim=0)\n",
    "        estimated_probability = torch.mean(probability_samples, dim=0)\n",
    "        assert estimated_probability.shape == (x.shape[0], 10)\n",
    "        assert torch.allclose(torch.sum(estimated_probability, dim=1), torch.tensor(1.0))\n",
    "        return estimated_probability\n",
    "\n",
    "class UnivariateGaussian(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Univariate Gaussian distribution.\n",
    "    For multivariate data, this assumes all elements to be i.i.d.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu: torch.Tensor, sigma: torch.Tensor):\n",
    "        super(UnivariateGaussian, self).__init__()  # always make sure to include the super-class init call!\n",
    "        assert mu.size() == () and sigma.size() == ()\n",
    "        assert sigma > 0\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement this\n",
    "        return 0.0\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        # TODO: Implement this\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class MultivariateDiagonalGaussian(ParameterDistribution):\n",
    "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor):\n",
    "        super(MultivariateDiagonalGaussian, self).__init__()\n",
    "        assert mu.size() == rho.size()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        my_dist = torch.distributions.Normal(self.mu, torch.exp(self.rho))\n",
    "        log_prob = my_dist.log_prob(values)\n",
    "        return log_prob.sum()\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        return self.mu + torch.randn_like(self.rho) * torch.exp(self.rho)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "92a3379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: Model, eval_loader: torch.utils.data.DataLoader, data_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Evaluate your model.\n",
    "    :param model: Trained model to evaluate\n",
    "    :param eval_loader: Data loader containing the training set for evaluation\n",
    "    :param data_dir: Data directory from which additional datasets are loaded\n",
    "    :param output_dir: Directory into which plots are saved\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict class probabilities on test data\n",
    "    predicted_probabilities = model.predict(eval_loader)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
    "    actual_classes = eval_loader.dataset.tensors[1].detach().numpy()\n",
    "    accuracy = np.mean((predicted_classes == actual_classes))\n",
    "    ece_score = ece(predicted_probabilities, actual_classes)\n",
    "    print(f'Accuracy: {accuracy.item():.3f}, ECE score: {ece_score:.3f}')\n",
    "\n",
    "    if EXTENDED_EVALUATION:\n",
    "        eval_samples = eval_loader.dataset.tensors[0].detach().numpy()\n",
    "\n",
    "        # Determine confidence per sample and sort accordingly\n",
    "        confidences = np.max(predicted_probabilities, axis=1)\n",
    "        sorted_confidence_indices = np.argsort(confidences)\n",
    "\n",
    "        # Plot samples your model is most confident about\n",
    "        print('Plotting most confident MNIST predictions')\n",
    "        most_confident_indices = sorted_confidence_indices[-10:]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = most_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(np.reshape(eval_samples[sample_idx], (28, 28)), cmap='gray')\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f'predicted {predicted_classes[sample_idx]}, actual {actual_classes[sample_idx]}')\n",
    "                bar_colors = ['C0'] * 10\n",
    "                bar_colors[actual_classes[sample_idx]] = 'C1'\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(10), predicted_probabilities[sample_idx], tick_label=np.arange(10), color=bar_colors\n",
    "                )\n",
    "        fig.suptitle('Most confident predictions', size=20)\n",
    "        fig.savefig(os.path.join(output_dir, 'mnist_most_confident.pdf'))\n",
    "\n",
    "        # Plot samples your model is least confident about\n",
    "        print('Plotting least confident MNIST predictions')\n",
    "        least_confident_indices = sorted_confidence_indices[:10]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = least_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(np.reshape(eval_samples[sample_idx], (28, 28)), cmap='gray')\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f'predicted {predicted_classes[sample_idx]}, actual {actual_classes[sample_idx]}')\n",
    "                bar_colors = ['C0'] * 10\n",
    "                bar_colors[actual_classes[sample_idx]] = 'C1'\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(10), predicted_probabilities[sample_idx], tick_label=np.arange(10), color=bar_colors\n",
    "                )\n",
    "        fig.suptitle('Least confident predictions', size=20)\n",
    "        fig.savefig(os.path.join(output_dir, 'mnist_least_confident.pdf'))\n",
    "\n",
    "        print('Plotting ambiguous and rotated MNIST confidences')\n",
    "        ambiguous_samples = torch.from_numpy(np.load(os.path.join(data_dir, 'test_x.npz'))['test_x']).reshape([-1, 784])[:10]\n",
    "        ambiguous_dataset = torch.utils.data.TensorDataset(ambiguous_samples, torch.zeros(10))\n",
    "        ambiguous_loader = torch.utils.data.DataLoader(\n",
    "            ambiguous_dataset, batch_size=10, shuffle=False, drop_last=False\n",
    "        )\n",
    "        ambiguous_predicted_probabilities = model.predict(ambiguous_loader)\n",
    "        ambiguous_predicted_classes = np.argmax(ambiguous_predicted_probabilities, axis=1)\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = 5 * row // 2 + col\n",
    "                ax[row, col].imshow(np.reshape(ambiguous_samples[sample_idx], (28, 28)), cmap='gray')\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f'predicted {ambiguous_predicted_classes[sample_idx]}')\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(10), ambiguous_predicted_probabilities[sample_idx], tick_label=np.arange(10)\n",
    "                )\n",
    "        fig.suptitle('Predictions on ambiguous and rotated MNIST', size=20)\n",
    "        fig.savefig(os.path.join(output_dir, 'ambiguous_rotated_mnist.pdf'))\n",
    "\n",
    "        # Do the same evaluation as on MNIST also on FashionMNIST\n",
    "        print('Predicting on FashionMNIST data')\n",
    "        fmnist_samples = torch.from_numpy(np.load(os.path.join(data_dir, 'fmnist.npz'))['x_test']).reshape([-1, 784])\n",
    "        fmnist_dataset = torch.utils.data.TensorDataset(fmnist_samples, torch.zeros(fmnist_samples.shape[0]))\n",
    "        fmnist_loader = torch.utils.data.DataLoader(\n",
    "            fmnist_dataset, batch_size=64, shuffle=False, drop_last=False\n",
    "        )\n",
    "        fmnist_predicted_probabilities = model.predict(fmnist_loader)\n",
    "        fmnist_predicted_classes = np.argmax(fmnist_predicted_probabilities, axis=1)\n",
    "        fmnist_confidences = np.max(fmnist_predicted_probabilities, axis=1)\n",
    "        fmnist_sorted_confidence_indices = np.argsort(fmnist_confidences)\n",
    "\n",
    "        # Plot FashionMNIST samples your model is most confident about\n",
    "        print('Plotting most confident FashionMNIST predictions')\n",
    "        most_confident_indices = fmnist_sorted_confidence_indices[-10:]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = most_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(np.reshape(fmnist_samples[sample_idx], (28, 28)), cmap='gray')\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f'predicted {fmnist_predicted_classes[sample_idx]}')\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(10), fmnist_predicted_probabilities[sample_idx], tick_label=np.arange(10)\n",
    "                )\n",
    "        fig.suptitle('Most confident predictions', size=20)\n",
    "        fig.savefig(os.path.join(output_dir, 'fashionmnist_most_confident.pdf'))\n",
    "\n",
    "        # Plot FashionMNIST samples your model is least confident about\n",
    "        print('Plotting least confident FashionMNIST predictions')\n",
    "        least_confident_indices = fmnist_sorted_confidence_indices[:10]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = least_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(np.reshape(fmnist_samples[sample_idx], (28, 28)), cmap='gray')\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f'predicted {fmnist_predicted_classes[sample_idx]}')\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(10), fmnist_predicted_probabilities[sample_idx], tick_label=np.arange(10)\n",
    "                )\n",
    "        fig.suptitle('Least confident predictions', size=20)\n",
    "        fig.savefig(os.path.join(output_dir, 'fashionmnist_least_confident.pdf'))\n",
    "\n",
    "        print('Determining suitability of your model for OOD detection')\n",
    "        all_confidences = np.concatenate([confidences, fmnist_confidences])\n",
    "        dataset_labels = np.concatenate([np.ones_like(confidences), np.zeros_like(fmnist_confidences)])\n",
    "        print(\n",
    "            'AUROC for MNIST vs. FashionMNIST OOD detection based on confidence: '\n",
    "            f'{roc_auc_score(dataset_labels, all_confidences):.3f}'\n",
    "        )\n",
    "        print(\n",
    "            'AUPRC for MNIST vs. FashionMNIST OOD detection based on confidence: '\n",
    "            f'{average_precision_score(dataset_labels, all_confidences):.3f}'\n",
    "        )\n",
    "    return accuracy , ece_score\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple module implementing a feedforward neural network.\n",
    "    You can use this model as a reference/baseline for calibration\n",
    "    in the normal neural network case.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, hidden_features: typing.Tuple[int, ...], out_features: int):\n",
    "        \"\"\"\n",
    "        Create a normal NN.\n",
    "        :param in_features: Number of input features\n",
    "        :param hidden_features: Tuple where each entry corresponds to a hidden layer with\n",
    "            the corresponding number of features.\n",
    "        :param out_features: Number of output features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        feature_sizes = (in_features,) + hidden_features + (out_features,)\n",
    "        num_affine_maps = len(feature_sizes) - 1\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(feature_sizes[idx], feature_sizes[idx + 1], bias=True)\n",
    "            for idx in range(num_affine_maps)\n",
    "        ])\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        current_features = x\n",
    "\n",
    "        for idx, current_layer in enumerate(self.layers):\n",
    "            new_features = current_layer(current_features)\n",
    "            if idx < len(self.layers) - 1:\n",
    "                new_features = self.activation(new_features)\n",
    "            current_features = new_features\n",
    "\n",
    "        return current_features\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert x.shape[1] == 28 ** 2\n",
    "        estimated_probability = F.softmax(self.forward(x), dim=1)\n",
    "        assert estimated_probability.shape == (x.shape[0], 10)\n",
    "        return estimated_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6a90e9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s, acc=0.102, loss=295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a BayesNet model\n",
      "initialising bayesian layer of size 784 x 100\n",
      "initialising bayesian layer of size 100 x 100\n",
      "initialising bayesian layer of size 100 x 10\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.42s/it, acc=0.891, loss=63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s, acc=0.117, loss=295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.835, ECE score: 0.049\n",
      "1.444080579416757\n",
      "Using a BayesNet model\n",
      "initialising bayesian layer of size 784 x 100\n",
      "initialising bayesian layer of size 100 x 100\n",
      "initialising bayesian layer of size 100 x 10\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.79s/it, acc=0.867, loss=52.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s, acc=0.117, loss=295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.864, ECE score: 0.041\n",
      "1.4959376677285818\n",
      "Using a BayesNet model\n",
      "initialising bayesian layer of size 784 x 100\n",
      "initialising bayesian layer of size 100 x 100\n",
      "initialising bayesian layer of size 100 x 10\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.78s/it, acc=0.891, loss=57.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s, acc=0.125, loss=295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.902, ECE score: 0.036\n",
      "1.5490005784363812\n",
      "Using a BayesNet model\n",
      "initialising bayesian layer of size 784 x 100\n",
      "initialising bayesian layer of size 100 x 100\n",
      "initialising bayesian layer of size 100 x 10\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.74s/it, acc=0.82, loss=68.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s, acc=0.133, loss=295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.874, ECE score: 0.040\n",
      "1.5075276695612865\n",
      "Using a BayesNet model\n",
      "initialising bayesian layer of size 784 x 100\n",
      "initialising bayesian layer of size 100 x 100\n",
      "initialising bayesian layer of size 100 x 10\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.80s/it, acc=0.836, loss=64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s, acc=0.117, loss=295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.893, ECE score: 0.035\n",
      "1.541937046944025\n",
      "Using a BayesNet model\n",
      "initialising bayesian layer of size 784 x 100\n",
      "initialising bayesian layer of size 100 x 100\n",
      "initialising bayesian layer of size 100 x 10\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.75s/it, acc=0.898, loss=43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s, acc=0.164, loss=295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.912, ECE score: 0.029\n",
      "1.577794051280665\n",
      "Using a BayesNet model\n",
      "initialising bayesian layer of size 784 x 100\n",
      "initialising bayesian layer of size 100 x 100\n",
      "initialising bayesian layer of size 100 x 10\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.91s/it, acc=0.883, loss=47.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s, acc=0.125, loss=296]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.916, ECE score: 0.033\n",
      "1.5696778828392783\n",
      "Using a BayesNet model\n",
      "initialising bayesian layer of size 784 x 100\n",
      "initialising bayesian layer of size 100 x 100\n",
      "initialising bayesian layer of size 100 x 10\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.86s/it, acc=0.859, loss=64.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s, acc=0.156, loss=357]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.897, ECE score: 0.071\n",
      "1.4482870881900602\n",
      "Using a BayesNet model\n",
      "initialising bayesian layer of size 784 x 100\n",
      "initialising bayesian layer of size 100 x 100\n",
      "initialising bayesian layer of size 100 x 10\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.18s/it, acc=0.0938, loss=294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on training data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-053084a3937d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Run actual solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrun_solution_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-169-99cc5a124811>\u001b[0m in \u001b[0;36mrun_solution_compare\u001b[0;34m(dataset_train, data_dir, output_dir)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             )\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mece\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mSCORE\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-25b8b0df57a0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, eval_loader, data_dir, output_dir)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Predict class probabilities on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpredicted_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Calculate evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-170-e3ba15556ee4>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data_loader)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprobability_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mcurrent_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mprobability_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_probabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-62d043b875a9>\u001b[0m in \u001b[0;36mpredict_probabilities\u001b[0;34m(self, x, num_mc_samples)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mc_samples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mprobability_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_mc_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mestimated_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mestimated_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-62d043b875a9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mc_samples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mprobability_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_mc_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mestimated_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mestimated_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-62d043b875a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mcurrent_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mlog_prior\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-62d043b875a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_var_posterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mlog_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_prior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mlog_variational_posterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_var_posterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-62d043b875a9>\u001b[0m in \u001b[0;36mlog_likelihood\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mmy_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_dir = os.curdir\n",
    "output_dir = os.curdir\n",
    "raw_train_data = np.load(os.path.join(data_dir, 'train_data.npz'))\n",
    "x_train = torch.from_numpy(raw_train_data['train_x']).reshape([-1, 784])\n",
    "y_train = torch.from_numpy(raw_train_data['train_y']).long()\n",
    "dataset_train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "\n",
    "    # Run actual solution\n",
    "run_solution_compare(dataset_train, data_dir=data_dir, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87cfd23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
